# Gerekli kÃ¼tÃ¼phaneleri import et
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard
import numpy as np
import matplotlib.pyplot as plt
import os
import json
from datetime import datetime
from collections import Counter
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

# 1. VERÄ° YÃœKLEME VE Ã–N Ä°ÅLEME
print("=" * 60)
print("1. VERÄ° YÃœKLEME VE Ã–N Ä°ÅLEME")
print("=" * 60)

# Veri dizinlerini tanÄ±mla
train_dir = './data/train'
valid_dir = './data/valid'

# Veri seti istatistiklerini kontrol et
def check_dataset_distribution(directory):
    class_counts = {}
    for class_name in os.listdir(directory):
        class_path = os.path.join(directory, class_name)
        if os.path.isdir(class_path):
            num_images = len([f for f in os.listdir(class_path) if f.endswith(('.jpg', '.jpeg', '.png'))])
            class_counts[class_name] = num_images
    return class_counts

print("\nVeri seti daÄŸÄ±lÄ±mÄ±nÄ± kontrol ediliyor...")
train_dist = check_dataset_distribution(train_dir)
valid_dist = check_dataset_distribution(valid_dir)

print(f"\nEÄŸitim seti daÄŸÄ±lÄ±mÄ±: {train_dist}")
print(f"DoÄŸrulama seti daÄŸÄ±lÄ±mÄ±: {valid_dist}")

# Minimum Ã¶rnek sayÄ±sÄ± kontrolÃ¼
min_samples = 300
for class_name, count in train_dist.items():
    if count < min_samples:
        print(f"âš ï¸ UYARI: '{class_name}' sÄ±nÄ±fÄ± sadece {count} Ã¶rnek iÃ§eriyor. En az {min_samples} Ã¶nerilir.")

# GeliÅŸmiÅŸ Data Augmentation
print("\nData augmentation pipeline oluÅŸturuluyor...")
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,  # ArttÄ±rÄ±ldÄ±
    width_shift_range=0.3,  # ArttÄ±rÄ±ldÄ±
    height_shift_range=0.3,  # ArttÄ±rÄ±ldÄ±
    shear_range=0.3,  # ArttÄ±rÄ±ldÄ±
    zoom_range=[0.8, 1.2],  # Hem yakÄ±nlaÅŸtÄ±rma hem uzaklaÅŸtÄ±rma
    horizontal_flip=True,
    vertical_flip=True,  # Yeni: dikey Ã§evirme
    brightness_range=[0.8, 1.2],  # Yeni: parlaklÄ±k ayarÄ±
    channel_shift_range=50.0,  # Yeni: renk kanalÄ± kaydÄ±rma
    fill_mode='nearest'
)

# DoÄŸrulama iÃ§in sadece normalizasyon
valid_datagen = ImageDataGenerator(rescale=1./255)

# Veri yÃ¼kleme
print("\nVeri yÃ¼kleniyor...")
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    shuffle=True,
    seed=42
)

validation_generator = valid_datagen.flow_from_directory(
    valid_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    shuffle=False
)

# SÄ±nÄ±f bilgileri
class_names = list(train_generator.class_indices.keys())
print(f"\nSÄ±nÄ±flar: {class_names}")
print(f"EÄŸitim Ã¶rnek sayÄ±sÄ±: {train_generator.samples}")
print(f"DoÄŸrulama Ã¶rnek sayÄ±sÄ±: {validation_generator.samples}")

# Class weights hesaplama (eÄŸer veri dengesizse)
print("\nClass weights hesaplanÄ±yor...")
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

class_weights = compute_class_weight(
    'balanced',
    classes=np.unique(train_generator.classes),
    y=train_generator.classes
)
class_weight_dict = dict(enumerate(class_weights))
print(f"Class weights: {class_weight_dict}")

# 2. TRANSFER LEARNING MODELÄ° OLUÅTURMA
print("\n" + "=" * 60)
print("2. TRANSFER LEARNING MODELÄ° OLUÅTURMA")
print("=" * 60)

# MobileNetV2 base model (hafif ve etkili)
print("MobileNetV2 base model yÃ¼kleniyor...")
base_model = MobileNetV2(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# Base model'i dondur (ilk eÄŸitimde)
base_model.trainable = False
print("Base model katmanlarÄ± donduruldu.")

# Yeni model oluÅŸtur
print("Yeni model mimarisi oluÅŸturuluyor...")
model = keras.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),  # Flatten yerine daha iyi
    layers.BatchNormalization(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.3),  # Dropout oranÄ± azaltÄ±ldÄ±
    layers.BatchNormalization(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.2),
    layers.BatchNormalization(),
    layers.Dense(1, activation='sigmoid')
])

# Model Ã¶zeti
model.summary()

# 3. MODEL DERLEME
print("\n" + "=" * 60)
print("3. MODEL DERLEME")
print("=" * 60)

# Learning Rate Scheduler
initial_learning_rate = 0.001

lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=10000,
    decay_rate=0.96,
    staircase=True
)

# Modeli derle
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),
    loss='binary_crossentropy',
    metrics=[
        'accuracy',
        keras.metrics.Precision(name='precision'),
        keras.metrics.Recall(name='recall'),
        keras.metrics.AUC(name='auc')
    ]
)

print("Model baÅŸarÄ±yla derlendi.")

# 4. CALLBACK'LERÄ° TANIMLA
print("\n" + "=" * 60)
print("4. CALLBACK'LER TANIMLANIYOR")
print("=" * 60)

# Callback'leri oluÅŸtur
callbacks = [
    # Early Stopping
    EarlyStopping(
        monitor='val_accuracy',
        patience=15,  # 15 epoch boyunca iyileÅŸme yoksa dur
        restore_best_weights=True,
        verbose=1
    ),
    
    # Model Checkpoint
    ModelCheckpoint(
        'best_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),
    
    # Learning Rate Reduction
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=7,
        min_lr=0.000001,
        verbose=1
    ),
    
    # TensorBoard (opsiyonel)
    # TensorBoard(log_dir='./logs', histogram_freq=1)
]

print(f"{len(callbacks)} callback tanÄ±mlandÄ±.")

# 5. MODEL EÄÄ°TÄ°MÄ° - FAZ 1
print("\n" + "=" * 60)
print("5. MODEL EÄÄ°TÄ°MÄ° - FAZ 1 (Base Model DondurulmuÅŸ)")
print("=" * 60)

# Ä°lk eÄŸitim (base model dondurulmuÅŸ)
epochs_phase1 = 30

print(f"Faz 1 eÄŸitimi baÅŸlatÄ±lÄ±yor: {epochs_phase1} epoch")
history = model.fit(
    train_generator,
    steps_per_epoch=max(1, train_generator.samples // train_generator.batch_size),
    epochs=epochs_phase1,
    validation_data=validation_generator,
    validation_steps=max(1, validation_generator.samples // validation_generator.batch_size),
    callbacks=callbacks,
    class_weight=class_weight_dict,
    verbose=1
)

# 6. FINE-TUNING - FAZ 2
print("\n" + "=" * 60)
print("6. FINE-TUNING - FAZ 2 (Base Model Ã‡Ã¶zÃ¼lÃ¼yor)")
print("=" * 60)

# Base model'in Ã¼st katmanlarÄ±nÄ± Ã§Ã¶z
base_model.trainable = True

# Sadece son 50 katmanÄ± eÄŸit (overfitting'i Ã¶nlemek iÃ§in)
for layer in base_model.layers[:100]:
    layer.trainable = False

print(f"Base model'in {len(base_model.layers) - 100} katmanÄ± Ã§Ã¶zÃ¼ldÃ¼.")

# Daha dÃ¼ÅŸÃ¼k learning rate ile yeniden derle
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.00001),  # Ã‡ok dÃ¼ÅŸÃ¼k LR
    loss='binary_crossentropy',
    metrics=['accuracy', 'precision', 'recall', 'auc']
)

# Fine-tuning eÄŸitimi
epochs_phase2 = 20
print(f"\nFine-tuning baÅŸlatÄ±lÄ±yor: {epochs_phase2} epoch")

history_fine = model.fit(
    train_generator,
    steps_per_epoch=max(1, train_generator.samples // train_generator.batch_size),
    epochs=epochs_phase2,
    validation_data=validation_generator,
    validation_steps=max(1, validation_generator.samples // validation_generator.batch_size),
    callbacks=callbacks,
    class_weight=class_weight_dict,
    verbose=1
)

# Ä°ki fazÄ±n history'sini birleÅŸtir
def combine_histories(h1, h2):
    combined = {}
    for key in h1.history.keys():
        combined[key] = h1.history[key] + h2.history[key]
    return combined

full_history = combine_histories(history, history_fine)

# 7. MODEL DEÄERLENDÄ°RMESÄ°
print("\n" + "=" * 60)
print("7. MODEL DEÄERLENDÄ°RMESÄ°")
print("=" * 60)

# En iyi modeli yÃ¼kle
print("En iyi model yÃ¼kleniyor...")
try:
    best_model = keras.models.load_model('best_model.h5')
    print("âœ“ En iyi model yÃ¼klendi")
except:
    best_model = model
    print("âš  En iyi model yÃ¼klenemedi, son model kullanÄ±lÄ±yor")

# KapsamlÄ± deÄŸerlendirme
print("\nModel doÄŸrulama setinde deÄŸerlendiriliyor...")
results = best_model.evaluate(validation_generator, verbose=0)

print("\n" + "=" * 40)
print("FÄ°NAL PERFORMANS METRÄ°KLERÄ°")
print("=" * 40)
print(f"KayÄ±p (Loss): {results[0]:.4f}")
print(f"DoÄŸruluk (Accuracy): {results[1]:.4f} (%{results[1]*100:.2f})")
print(f"Precision: {results[2]:.4f}")
print(f"Recall: {results[3]:.4f}")
print(f"AUC: {results[4]:.4f}")

# 8. KAPSAMLI GÃ–RSELLEÅTÄ°RME
print("\n" + "=" * 60)
print("8. KAPSAMLI GÃ–RSELLEÅTÄ°RME")
print("=" * 60)

plt.figure(figsize=(20, 8))

# 1. KayÄ±p GrafiÄŸi
plt.subplot(2, 3, 1)
plt.plot(full_history['loss'], label='EÄŸitim KaybÄ±', linewidth=2)
plt.plot(full_history['val_loss'], label='DoÄŸrulama KaybÄ±', linewidth=2)
plt.title('Epoch BaÅŸÄ±na Model KaybÄ±', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('KayÄ±p')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axvline(x=epochs_phase1, color='r', linestyle='--', alpha=0.5, label='Fine-tuning BaÅŸlangÄ±cÄ±')
plt.legend()

# 2. DoÄŸruluk GrafiÄŸi
plt.subplot(2, 3, 2)
plt.plot(full_history['accuracy'], label='EÄŸitim DoÄŸruluÄŸu', linewidth=2)
plt.plot(full_history['val_accuracy'], label='DoÄŸrulama DoÄŸruluÄŸu', linewidth=2)
plt.title('Epoch BaÅŸÄ±na Model DoÄŸruluÄŸu', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('DoÄŸruluk')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axvline(x=epochs_phase1, color='r', linestyle='--', alpha=0.5)
plt.axhline(y=0.90, color='g', linestyle=':', alpha=0.5, label='%90 Hedefi')
plt.legend()

# 3. Precision-Recall GrafiÄŸi
plt.subplot(2, 3, 3)
plt.plot(full_history['precision'], label='Precision', linewidth=2)
plt.plot(full_history['recall'], label='Recall', linewidth=2)
plt.title('Precision ve Recall GeliÅŸimi', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('DeÄŸer')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axvline(x=epochs_phase1, color='r', linestyle='--', alpha=0.5)

# 4. AUC GrafiÄŸi
plt.subplot(2, 3, 4)
plt.plot(full_history['auc'], label='AUC', linewidth=2, color='purple')
plt.title('AUC GeliÅŸimi', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('AUC')
plt.legend()
plt.grid(True, alpha=0.3)
plt.axvline(x=epochs_phase1, color='r', linestyle='--', alpha=0.5)

# 5. Learning Rate GeliÅŸimi
plt.subplot(2, 3, 5)
if 'lr' in full_history:
    plt.plot(full_history['lr'], label='Learning Rate', linewidth=2, color='orange')
    plt.title('Learning Rate GeliÅŸimi', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate')
    plt.yscale('log')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.axvline(x=epochs_phase1, color='r', linestyle='--', alpha=0.5)

# 6. Confusion Matrix (tahmini)
plt.subplot(2, 3, 6)
# Tahminler
validation_generator.reset()
y_pred = best_model.predict(validation_generator, verbose=0)
y_pred = (y_pred > 0.5).astype(int).flatten()
y_true = validation_generator.classes[:len(y_pred)]

# Confusion matrix hesapla
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
plt.ylabel('GerÃ§ek DeÄŸer')
plt.xlabel('Tahmin Edilen')

plt.tight_layout()
plt.savefig('improved_training_history.png', dpi=300, bbox_inches='tight')
plt.show()

# 9. DETAYLI SÄ±nÄ±flandÄ±rma Raporu
print("\n" + "=" * 60)
print("9. DETAYLI SINIFLANDIRMA RAPORU")
print("=" * 60)

print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=class_names))

# 10. Ã–RNEK TAHMÄ°NLER
print("\n" + "=" * 60)
print("10. Ã–RNEK TAHMÄ°NLER")
print("=" * 60)

# Rastgele Ã¶rnekler seÃ§
num_examples = 5
validation_generator.reset()
sample_images = []
sample_labels = []

for i in range(num_examples):
    img, label = next(validation_generator)
    sample_images.append(img[0])
    sample_labels.append(label[0])

sample_images = np.array(sample_images)
sample_labels = np.array(sample_labels)

# Tahmin yap
predictions = best_model.predict(sample_images, verbose=0)

print("\nÃ–rnek Tahmin SonuÃ§larÄ±:")
print("-" * 50)
for i in range(num_examples):
    actual_class = class_names[0] if sample_labels[i] == 0 else class_names[1]
    pred_prob = predictions[i][0]
    pred_class = class_names[0] if pred_prob < 0.5 else class_names[1]
    confidence = pred_prob if pred_class == class_names[1] else 1 - pred_prob
    
    # DoÄŸru/yanlÄ±ÅŸ renkli gÃ¶sterim
    if actual_class == pred_class:
        status = "âœ“ DOÄRU"
        color = "\033[92m"  # YeÅŸil
    else:
        status = "âœ— YANLIÅ"
        color = "\033[91m"  # KÄ±rmÄ±zÄ±
    
    print(f"{color}Ã–rnek {i+1}:")
    print(f"  GerÃ§ek: {actual_class}")
    print(f"  Tahmin: {pred_class} (%{confidence*100:.2f} gÃ¼ven)")
    print(f"  Durum: {status}\033[0m")
    print()

# 11. MODEL VE SONUÃ‡LARI KAYDETME
print("\n" + "=" * 60)
print("11. MODEL VE SONUÃ‡LARI KAYDETME")
print("=" * 60)

# DetaylÄ± sonuÃ§larÄ± kaydet
training_results = {
    'model_name': 'improved_car_truck_classifier',
    'model_architecture': 'MobileNetV2 + Custom Head',
    'training_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    'class_names': class_names,
    'class_distribution_train': train_dist,
    'class_distribution_val': valid_dist,
    'training_samples': train_generator.samples,
    'validation_samples': validation_generator.samples,
    'phase1_epochs': epochs_phase1,
    'phase2_epochs': epochs_phase2,
    'batch_size': 32,
    'final_metrics': {
        'loss': float(results[0]),
        'accuracy': float(results[1]),
        'precision': float(results[2]),
        'recall': float(results[3]),
        'auc': float(results[4])
    },
    'training_history': {
        'loss': [float(x) for x in full_history['loss']],
        'accuracy': [float(x) for x in full_history['accuracy']],
        'val_loss': [float(x) for x in full_history['val_loss']],
        'val_accuracy': [float(x) for x in full_history['val_accuracy']],
        'precision': [float(x) for x in full_history['precision']],
        'recall': [float(x) for x in full_history['recall']],
        'auc': [float(x) for x in full_history['auc']]
    }
}

# JSON olarak kaydet
with open('improved_training_results.json', 'w', encoding='utf-8') as f:
    json.dump(training_results, f, indent=4, ensure_ascii=False)

# Modeli kaydet
best_model.save('final_improved_model.h5')

print("\n" + "=" * 60)
print("EÄÄ°TÄ°M TAMAMLANDI!")
print("=" * 60)
print("\nâœ“ Kaydedilen dosyalar:")
print("  1. final_improved_model.h5 - EÄŸitilmiÅŸ model")
print("  2. best_model.h5 - En iyi performanslÄ± model")
print("  3. improved_training_results.json - DetaylÄ± sonuÃ§lar")
print("  4. improved_training_history.png - GeliÅŸmiÅŸ grafikler")

print(f"\nâœ“ Final DoÄŸruluk: %{results[1]*100:.2f}")
print(f"âœ“ Final KayÄ±p: {results[0]:.4f}")

if results[1] >= 0.90:
    print("\nğŸ‰ TEBRÄ°KLER! Model %90+ doÄŸruluk hedefine ulaÅŸtÄ±!")
elif results[1] >= 0.85:
    print("\nğŸ‘ Ä°YÄ°! Model %85+ doÄŸrulukta.")
else:
    print(f"\nâš  GELÄ°ÅTÄ°RME GEREKÄ°YOR: DoÄŸruluk %85'in altÄ±nda. Veri setini artÄ±rmayÄ± deneyin.")

print("\n" + "=" * 60)
print("Ä°YÄ°LEÅTÄ°RME Ã–ZETÄ°:")
print("=" * 60)
print("âœ“ Transfer Learning (MobileNetV2)")
print("âœ“ GeliÅŸmiÅŸ Data Augmentation")
print("âœ“ 2-FazlÄ± EÄŸitim (Dondurma + Fine-tuning)")
print("âœ“ Learning Rate Scheduling")
print("âœ“ Early Stopping ve Model Checkpoint")
print("âœ“ Class Weight Balancing")
print("âœ“ Batch Normalization katmanlarÄ±")
print("âœ“ GlobalAveragePooling kullanÄ±mÄ±")
print("âœ“ Ã‡oklu metrik takibi (Accuracy, Precision, Recall, AUC)")

print("\nModel hazÄ±r! ğŸš€")